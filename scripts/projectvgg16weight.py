# -*- coding: utf-8 -*-
"""ProjectVGG16weight.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16eCPZTP6X45Nw1aYC87ow9pccUq5h9jC
"""

import os
os.environ['KAGGLE_CONFIG_DIR']='/content'

!kaggle competitions download -c severstal-steel-defect-detection

!unzip \*.zip && rm *.zip

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import matplotlib.pyplot as plt
from sklearn import preprocessing
import keras
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import cv2
from sklearn.preprocessing import OneHotEncoder,LabelEncoder
from keras.utils import to_categorical
from keras.models import Sequential
import tensorflow as tf
from keras.layers import Convolution2D, Dropout, Dense,MaxPooling2D
from keras.layers import Dense,Conv2D,Flatten,MaxPooling2D,Dropout
from keras.layers import BatchNormalization
from keras.layers import MaxPooling2D
from keras.layers import Flatten

# Load CSV file for labels
data = pd.read_csv("/content/train.csv")

data.shape

data["ClassId"].value_counts().plot(kind = 'bar')
data["ClassId"].value_counts()

# After loading CSV file for labels
class_counts = data["ClassId"].value_counts()
total_count = class_counts.sum()
num_classes = len(class_counts)
weight_dictionary = {class_id: total_count / count for class_id, count in class_counts.items()}

# Convert class IDs to indices starting from 0
class_indices = range(num_classes)
class_weight_dict = {index: weight_dictionary.get(class_id, 1.0) for class_id, index in zip(class_counts.index, class_indices)}
print(class_weight_dict)

l1=[]
l2=[]
for img,ClassId,EncodedPixels in tqdm(data.values):
    image=cv2.imread("/content/train_images/{}".format(img),cv2.IMREAD_COLOR)
    image=cv2.resize(image,(120,120))
    l1.append(image)
    l2.append(ClassId)

encoder = LabelEncoder()

X= np.array(l1)
X = X/255

y = encoder.fit_transform(l2)
y = to_categorical(y)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, shuffle=True, random_state=42)
print("x_train shape:",X_train.shape)
print("x_test shape:",X_test.shape)
print("y_train shape:",y_train.shape)
print("y_test shape:",y_test.shape)

from keras.applications import vgg16

img_rows, img_cols = 120, 120

vgg = vgg16.VGG16(weights = 'imagenet',
                 include_top = False,
                 input_shape = (img_rows, img_cols, 3))

# Here we freeze the last 4 layers
# Layers are set to trainable as True by default
for layer in vgg.layers:
    layer.trainable = False

# Let's print our layers
for (i,layer) in enumerate(vgg.layers):
    print(str(i) + " "+ layer.__class__.__name__, layer.trainable)

def lw(bottom_model, num_classes):
    """creates the top or head of the model that will be
    placed ontop of the bottom layers"""

    top_model = bottom_model.output
    top_model = GlobalAveragePooling2D()(top_model)
    top_model = Dense(1024,activation='relu')(top_model)
    top_model = Dense(1024,activation='relu')(top_model)
    top_model = Dense(512,activation='relu')(top_model)
    top_model = Dense(num_classes,activation='softmax')(top_model)
    return top_model

from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D
from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D

from keras.models import Model


num_classes = 4

FC_Head = lw(vgg, num_classes)

model = Model(inputs = vgg.input, outputs = FC_Head)

print(model.summary())

from tensorflow.keras.models import Model
model.compile(optimizer='adam', loss = 'categorical_crossentropy',metrics = ['accuracy'])

from keras.callbacks import EarlyStopping

early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Modify the model training phase to pass class weights
history = model.fit(X_train, y_train,
                    epochs=15,
                    validation_data=(X_test, y_test),
                    verbose=1,
                    initial_epoch=0,
                    callbacks=[early_stopping],
                    class_weight=class_weight_dict)

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'r', label='Training accuracy')
plt.plot(epochs, val_acc, 'b', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend(loc=0)
plt.figure()

plt.show()

y_prediction = model.predict(X_test)
y_prediction = np.argmax(y_prediction, axis=1)
y_test = np.argmax(y_test, axis=1)

from sklearn.metrics import classification_report

classification_report_ = classification_report(y_test, y_prediction)
print("Classification Report:")
print(classification_report_)

from sklearn.metrics import confusion_matrix

confusion_mtx = confusion_matrix(y_test, y_prediction)
print("Confusion Matrix:")
print(confusion_mtx)

# Save the trained model
model.save("Vgg16_weight_model.h5")

